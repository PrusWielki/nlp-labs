{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP - Extracting product info from texts with LLM**\n",
    "\n",
    "- *Karina Tiurina*\n",
    "- *Salveen Dutt*\n",
    "- *Patryk Prusak*\n",
    "\n",
    "Comparison of various NLP models in the task of classifying reviews to specific product types, extracting product keywords and attributes.\n",
    "\n",
    "\n",
    "Metrics used:\n",
    "\n",
    "1. Smith-Waterman\n",
    "2. Needleman-Wunsch\n",
    "3. Levenshtein Distance\n",
    "4. Cosine Similarity\n",
    "5. Bert Score\n",
    "6. Custom Metric\n",
    "\n",
    "Models used:\n",
    "\n",
    "1. Qwen2.5-1.5B\n",
    "2. gemma-2-2b\n",
    "3. Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs & Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\Bio\\pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bert_score import score\n",
    "from transformers import pipeline\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein\n",
    "from Bio import pairwise2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_theme(palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"products.json\", \"r\") as file:\n",
    "    products = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main thing: on the battery itself, driving calmly, not exceeding 30 km/h, you can do 60-70km.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[\"products\"][\"Electric Bike\"][\"reviews\"][0][\"review_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_product_review_similarity(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates a similarity score between provided data and a llm answer based on\n",
    "    categories, brands, and keywords, including a comparison of full product\n",
    "    title with review information using BERTScore.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # String Comparison (BERTScore) between Product Title and Review Data\n",
    "    review_info_string = \" \".join(\n",
    "        [\n",
    "            original_data[\"golden_answer\"][\"product_category\"],\n",
    "            \" \".join(original_data[\"golden_answer\"][\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    product_info_string = \" \".join(\n",
    "        [\n",
    "            llm_answer[\"product_category\"][\"type\"],\n",
    "            \" \".join(llm_answer[\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        [product_info_string],\n",
    "        [review_info_string],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(F1.mean().item())\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def bert_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the BERTScore between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(\n",
    "        [\"\".join(review_keywords)],\n",
    "        [\"\".join(product_keywords)],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def cosine_similarity_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        1 - cosine(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "    ) * 0.2 + (\n",
    "        1 - cosine(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "    ) * 0.3\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def levenshtein_distance(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        Levenshtein.distance(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        Levenshtein.distance(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def needleman_wunsch_similarity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between two sequences.\n",
    "    \"\"\"\n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "    max_score = max(alignment.score for alignment in alignments)\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def needleman_wunsch(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between provided data and a llm answer\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Needleman-Wunsch Similarity\n",
    "    similarity_score += (\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_keywords), \" \".join(product_keywords)\n",
    "        )\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_attributes), \" \".join(product_attributes)\n",
    "        )\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def smith_waterman_similarity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculates the Smith-Waterman similarity between two sequences.\n",
    "    \"\"\"\n",
    "    match = 2\n",
    "    mismatch = -1\n",
    "    gap = -1\n",
    "\n",
    "    # Initialize the scoring matrix\n",
    "    m, n = len(seq1), len(seq2)\n",
    "    score_matrix = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    max_score = 0\n",
    "\n",
    "    # Fill the scoring matrix\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                score = match\n",
    "            else:\n",
    "                score = mismatch\n",
    "            score_matrix[i][j] = max(\n",
    "                0,\n",
    "                score_matrix[i - 1][j - 1] + score,\n",
    "                score_matrix[i - 1][j] + gap,\n",
    "                score_matrix[i][j - 1] + gap,\n",
    "            )\n",
    "            max_score = max(max_score, score_matrix[i][j])\n",
    "\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def smith_waterman(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Smith-Waterman similarity between provided data and a llm answer\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Smith-Waterman Similarity\n",
    "    similarity_score += (\n",
    "        smith_waterman_similarity(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "    +(smith_waterman_similarity(review_attributes, product_attributes) * 0.3)\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant, helping in understanding of reviews. Carefully read the review:\n",
    "{content}\n",
    "\n",
    "Return json format with the following JSON schema:\n",
    "\n",
    "{{\n",
    "        \"product_category\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Electric bicycle\", \"Refrigirator\", \"The Blocks\", \"Others\"]\n",
    "        }},\n",
    "        \"product_attributes\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "        \"other_keywords\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly inspired by bioinformatics (https://en.wikipedia.org/wiki/Sequence_alignment)\n",
    "similarity_metrics = [\n",
    "    smith_waterman,\n",
    "    needleman_wunsch,\n",
    "    levenshtein_distance,\n",
    "    cosine_similarity,\n",
    "    bert_score,\n",
    "    compare_product_review_similarity,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration for model selection taken from https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-best-models-652d6c7965a4619fb5c27a03\n",
    "\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using google/gemma-2-2b-it...\n",
      "Generating responses for Refrigirator reviews using google/gemma-2-2b-it...\n",
      "Generating responses for The LEGO reviews using google/gemma-2-2b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Refrigirator reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for The LEGO reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "\n",
    "        # load in 4bit greatly reduces the memory usage\n",
    "        nlp = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            model_kwargs={\n",
    "                \"torch_dtype\": torch.bfloat16,\n",
    "                \"quantization_config\": {\n",
    "                    \"load_in_4bit\": True,\n",
    "                },\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model}, error: {e}\")\n",
    "        continue\n",
    "    for productType in products[\"products\"].keys():\n",
    "        print(f\"Generating responses for {productType} reviews using {model}...\")\n",
    "        for review in products[\"products\"][productType][\"reviews\"]:\n",
    "            try:\n",
    "                reviews_content = review[\"review_content\"]\n",
    "\n",
    "                prompt = prompt_template.format(content=reviews_content)\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                outputs = nlp(messages, max_new_tokens=256)\n",
    "                responses.append(\n",
    "                    [outputs[0][\"generated_text\"][1][\"content\"], review, model]\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed to generate response for {productType} review, error: {e}\"\n",
    "                )\n",
    "                continue\n",
    "    try:\n",
    "        # the memory is probably freed up, but to be safe we delete the nlp object and empty the cache\n",
    "        del nlp\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete nlp object, error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in responses:\n",
    "    # try:\n",
    "    llm_response = response[0]\n",
    "    if not isinstance(llm_response, dict):\n",
    "        llm_response = json.loads(\n",
    "            llm_response.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "        )\n",
    "    score = [metric(response[1], llm_response) for metric in similarity_metrics]\n",
    "    response.extend(score)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to calculate similarity score, error: {e}\")\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"rb\") as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"response\", \"review\", \"model\"] + [\n",
    "    metric.__name__ for metric in similarity_metrics\n",
    "]\n",
    "results_df = pd.DataFrame(responses, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the barplots, we need to transform the results, each row should contain a model name, a metric name, and the similarity score\n",
    "# Then plot x-axis as model name, y-axis as similarity score, and hue as metric name\n",
    "\n",
    "# Transform the results#\n",
    "results_transformed = results_df.melt(\n",
    "    id_vars=[\"response\", \"review\", \"model\"],\n",
    "    var_name=\"similarity_metric\",\n",
    "    value_name=\"similarity_score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
