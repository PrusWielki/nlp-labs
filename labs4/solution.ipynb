{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP - Extracting product info from texts with LLM**\n",
    "\n",
    "- *Karina Tiurina*\n",
    "- *Salveen Dutt*\n",
    "- *Patryk Prusak*\n",
    "\n",
    "Comparison of various NLP models in the task of classifying reviews to specific product types, extracting product keywords and attributes.\n",
    "\n",
    "\n",
    "Metrics used:\n",
    "\n",
    "1. Smith-Waterman\n",
    "2. Needleman-Wunsch\n",
    "3. Levenshtein Distance\n",
    "4. Cosine Similarity\n",
    "5. Bert Score\n",
    "6. Custom Metric\n",
    "\n",
    "Models used:\n",
    "\n",
    "1. Qwen2.5-1.5B\n",
    "2. gemma-2-2b\n",
    "3. Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs & Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\Bio\\pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bert_score import score\n",
    "from transformers import pipeline\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein\n",
    "from Bio import pairwise2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_theme(palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"products.json\", \"r\") as file:\n",
    "    products = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main thing: on the battery itself, driving calmly, not exceeding 30 km/h, you can do 60-70km.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[\"products\"][\"Electric Bike\"][\"reviews\"][0][\"review_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_product_review_similarity(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates a similarity score between provided data and a llm answer based on\n",
    "    categories, brands, and keywords, including a comparison of full product\n",
    "    title with review information using BERTScore.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # String Comparison (BERTScore) between Product Title and Review Data\n",
    "    review_info_string = \" \".join(\n",
    "        [\n",
    "            original_data[\"golden_answer\"][\"product_category\"],\n",
    "            \" \".join(original_data[\"golden_answer\"][\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    product_info_string = \" \".join(\n",
    "        [\n",
    "            llm_answer[\"product_category\"][\"type\"],\n",
    "            \" \".join(llm_answer[\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        [product_info_string],\n",
    "        [review_info_string],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(F1.mean().item())\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def bert_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the BERTScore between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(\n",
    "        [\"\".join(review_keywords)],\n",
    "        [\"\".join(product_keywords)],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def cosine_similarity_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        1 - cosine(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "    ) * 0.2 + (\n",
    "        1 - cosine(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "    ) * 0.3\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def levenshtein_distance(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        Levenshtein.distance(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        Levenshtein.distance(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def needleman_wunsch_similarity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between two sequences.\n",
    "    \"\"\"\n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "    max_score = max(alignment.score for alignment in alignments)\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def needleman_wunsch(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between provided data and a llm answer\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Needleman-Wunsch Similarity\n",
    "    similarity_score += (\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_keywords), \" \".join(product_keywords)\n",
    "        )\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_attributes), \" \".join(product_attributes)\n",
    "        )\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def smith_waterman_similarity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculates the Smith-Waterman similarity between two sequences.\n",
    "    \"\"\"\n",
    "    match = 2\n",
    "    mismatch = -1\n",
    "    gap = -1\n",
    "\n",
    "    # Initialize the scoring matrix\n",
    "    m, n = len(seq1), len(seq2)\n",
    "    score_matrix = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    max_score = 0\n",
    "\n",
    "    # Fill the scoring matrix\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                score = match\n",
    "            else:\n",
    "                score = mismatch\n",
    "            score_matrix[i][j] = max(\n",
    "                0,\n",
    "                score_matrix[i - 1][j - 1] + score,\n",
    "                score_matrix[i - 1][j] + gap,\n",
    "                score_matrix[i][j - 1] + gap,\n",
    "            )\n",
    "            max_score = max(max_score, score_matrix[i][j])\n",
    "\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def smith_waterman(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Smith-Waterman similarity between provided data and a llm answer\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"][\"type\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Smith-Waterman Similarity\n",
    "    similarity_score += (\n",
    "        smith_waterman_similarity(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "    +(smith_waterman_similarity(review_attributes, product_attributes) * 0.3)\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant, helping in understanding of reviews. Carefully read the review:\n",
    "{content}\n",
    "\n",
    "Return json format with the following JSON schema:\n",
    "\n",
    "{{\n",
    "        \"product_category\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Electric bicycle\", \"Refrigirator\", \"The Blocks\", \"Others\"]\n",
    "        }},\n",
    "        \"product_attributes\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "        \"other_keywords\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly inspired by bioinformatics (https://en.wikipedia.org/wiki/Sequence_alignment)\n",
    "similarity_metrics = [\n",
    "    smith_waterman,\n",
    "    needleman_wunsch,\n",
    "    levenshtein_distance,\n",
    "    cosine_similarity,\n",
    "    bert_score,\n",
    "    compare_product_review_similarity,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration for model selection taken from https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-best-models-652d6c7965a4619fb5c27a03\n",
    "\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    \"google/gemma-2-2b\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-1.5B...\n",
      "Failed to load model google/gemma-2-9b, error: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2-9b.\n",
      "403 Client Error. (Request ID: Root=1-6727fb89-381b7ed509d6632e614f2d64;9c96183e-3242-4448-8473-f5ff1163e417)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2-9b/resolve/main/config.json.\n",
      "Access to model google/gemma-2-9b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-2-9b to ask for access.\n",
      "Failed to load model google/gemma-2-2b, error: name 'nlp' is not defined\n",
      "Failed to load model meta-llama/Llama-3.2-3B-Instruct, error: name 'nlp' is not defined\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "\n",
    "        # load in 4bit greatly reduces the memory usage\n",
    "        nlp = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            model_kwargs={\n",
    "                \"quantization_config\": {\n",
    "                    \"load_in_4bit\": True,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model}, error: {e}\")\n",
    "        continue\n",
    "    for productType in products[\"products\"].keys():\n",
    "        print(f\"Generating responses for {productType} reviews using {model}...\")\n",
    "        for review in products[\"products\"][productType][\"reviews\"]:\n",
    "            try:\n",
    "                reviews_content = review[\"review_content\"]\n",
    "\n",
    "                prompt = prompt_template.format(content=reviews_content)\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                response = nlp(messages, max_new_tokens=1024, num_return_sequences=1)\n",
    "                responses.append(\n",
    "                    [response[0][\"generated_text\"][1][\"content\"], review, model]\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed to generate response for {productType} review, error: {e}\"\n",
    "                )\n",
    "                continue\n",
    "        try:\n",
    "            # the memory is probably freed up, but to be safe we delete the nlp object and empty the cache\n",
    "            del nlp\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete nlp object, error: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m      5\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m      6\u001b[0m         llm_response\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msimilarity_metrics\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m response\u001b[38;5;241m.\u001b[39mextend(score)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     print(f\"Failed to calculate similarity score, error: {e}\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m      5\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m      6\u001b[0m         llm_response\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[0m score \u001b[38;5;241m=\u001b[39m [\u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m similarity_metrics]\n\u001b[0;32m      9\u001b[0m response\u001b[38;5;241m.\u001b[39mextend(score)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     print(f\"Failed to calculate similarity score, error: {e}\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:185\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    175\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    176\u001b[0m         X,\n\u001b[0;32m    177\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    195\u001b[0m         Y,\n\u001b[0;32m    196\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    202\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:745\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    743\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "    # try:\n",
    "    llm_response = response[0]\n",
    "    if not isinstance(llm_response, dict):\n",
    "        llm_response = json.loads(\n",
    "            llm_response.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "        )\n",
    "    score = [metric(response[1], llm_response) for metric in similarity_metrics]\n",
    "    response.extend(score)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to calculate similarity score, error: {e}\")\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"rb\") as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "9 columns passed, passed data had 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 9 columns passed, passed data had 3 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m      2\u001b[0m     metric\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m similarity_metrics\n\u001b[0;32m      3\u001b[0m ]\n\u001b[1;32m----> 4\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\Documents\\Repos\\nlp-labs\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 9 columns passed, passed data had 3 columns"
     ]
    }
   ],
   "source": [
    "columns = [\"response\", \"review\", \"model\"] + [\n",
    "    metric.__name__ for metric in similarity_metrics\n",
    "]\n",
    "results_df = pd.DataFrame(responses, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the barplots, we need to transform the results, each row should contain a model name, a metric name, and the similarity score\n",
    "# Then plot x-axis as model name, y-axis as similarity score, and hue as metric name\n",
    "\n",
    "# Transform the results#\n",
    "results_transformed = results_df.melt(\n",
    "    id_vars=[\"response\", \"review\", \"model\"],\n",
    "    var_name=\"similarity_metric\",\n",
    "    value_name=\"similarity_score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
