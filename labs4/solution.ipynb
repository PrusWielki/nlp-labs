{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP - Extracting product info from texts with LLM**\n",
    "\n",
    "- *Karina Tiurina*\n",
    "- *Salveen Dutt*\n",
    "- *Patryk Prusak*\n",
    "\n",
    "Comparison of various NLP models in the task of classifying reviews to specific product types, extracting product keywords and attributes.\n",
    "\n",
    "\n",
    "Metrics used:\n",
    "\n",
    "1. Smith-Waterman\n",
    "2. Needleman-Wunsch\n",
    "3. Levenshtein Distance\n",
    "4. Cosine Similarity\n",
    "5. Bert Score\n",
    "6. Custom Metric\n",
    "\n",
    "Models used:\n",
    "\n",
    "1. Qwen2.5-1.5B\n",
    "2. gemma-2-2b\n",
    "3. Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs & Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bert_score import score\n",
    "from transformers import pipeline\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein\n",
    "from Bio import pairwise2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_theme(palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"products.json\", \"r\") as file:\n",
    "    products = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main thing: on the battery itself, driving calmly, not exceeding 30 km/h, you can do 60-70km.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[\"products\"][\"Electric Bike\"][\"reviews\"][0][\"review_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_product_review_similarity(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates a similarity score between provided data and a llm answer based on\n",
    "    categories, brands, and keywords, including a comparison of full product\n",
    "    title with review information using BERTScore.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if original_data.get(\"golden_answer\", {}).get(\"product_category\") == llm_answer.get(\n",
    "        \"product_category\"\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # String Comparison (BERTScore) between Product Title and Review Data\n",
    "    review_info_string = \" \".join(\n",
    "        [\n",
    "            original_data[\"golden_answer\"][\"product_category\"],\n",
    "            \" \".join(original_data[\"golden_answer\"][\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    product_info_string = \" \".join(\n",
    "        [\n",
    "            llm_answer[\"product_category\"],\n",
    "            \" \".join(llm_answer[\"other_keywords\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        [product_info_string],\n",
    "        [review_info_string],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(F1.mean().item())\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def bert_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the BERTScore between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data.get(\"golden_answer\", {}).get(\"other_keywords\", [])\n",
    "    product_keywords = llm_answer.get(\"other_keywords\", [])\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(\n",
    "        [\"\".join(review_keywords)],\n",
    "        [\"\".join(product_keywords)],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def cosine_similarity_score(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data.get(\"golden_answer\", {}).get(\n",
    "        \"product_attributes\", []\n",
    "    )\n",
    "    product_attributes = llm_answer.get(\"product_attributes\", [])\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        1 - cosine(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "    ) * 0.2 + (\n",
    "        1 - cosine(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "    ) * 0.3\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def levenshtein_distance(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between provided data and a llm answer based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    # Attribute Matching (Medium weight)\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    similarity_score += (\n",
    "        Levenshtein.distance(\" \".join(review_keywords), \" \".join(product_keywords))\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        Levenshtein.distance(\" \".join(review_attributes), \" \".join(product_attributes))\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def needleman_wunsch_similarity(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between two sequences.\n",
    "    \"\"\"\n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "    max_score = max(alignment.score for alignment in alignments)\n",
    "    return max_score\n",
    "\n",
    "\n",
    "def needleman_wunsch(original_data, llm_answer):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between provided data and a llm answer\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if (\n",
    "        original_data[\"golden_answer\"][\"product_category\"]\n",
    "        == llm_answer[\"product_category\"]\n",
    "    ):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = original_data[\"golden_answer\"][\"other_keywords\"]\n",
    "    product_keywords = llm_answer[\"other_keywords\"]\n",
    "\n",
    "    review_attributes = original_data[\"golden_answer\"][\"product_attributes\"]\n",
    "    product_attributes = llm_answer[\"product_attributes\"]\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Needleman-Wunsch Similarity\n",
    "    similarity_score += (\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_keywords), \" \".join(product_keywords)\n",
    "        )\n",
    "        * 0.2\n",
    "    )\n",
    "    +(\n",
    "        needleman_wunsch_similarity(\n",
    "            \" \".join(review_attributes), \" \".join(product_attributes)\n",
    "        )\n",
    "        * 0.3\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant, helping in understanding of reviews. Carefully read the review:\n",
    "{content}\n",
    "\n",
    "Return json format with the following JSON schema:\n",
    "\n",
    "{{\n",
    "        \"product_category\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Electric bicycle\", \"Refrigirator\", \"The Blocks\", \"Others\"]\n",
    "        }},\n",
    "        \"product_attributes\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "        \"other_keywords\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partly inspired by bioinformatics (https://en.wikipedia.org/wiki/Sequence_alignment)\n",
    "similarity_metrics = [\n",
    "    needleman_wunsch,\n",
    "    levenshtein_distance,\n",
    "    # cosine_similarity,\n",
    "    bert_score,\n",
    "    compare_product_review_similarity,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration for model selection taken from https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-best-models-652d6c7965a4619fb5c27a03\n",
    "\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-1.5B...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using google/gemma-2-2b-it...\n",
      "Generating responses for Refrigirator reviews using google/gemma-2-2b-it...\n",
      "Generating responses for The LEGO reviews using google/gemma-2-2b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Refrigirator reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for The LEGO reviews using meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "\n",
    "        # load in 4bit greatly reduces the memory usage\n",
    "        nlp = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            model_kwargs={\n",
    "                \"torch_dtype\": torch.bfloat16,\n",
    "                \"quantization_config\": {\n",
    "                    \"load_in_4bit\": True,\n",
    "                },\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model}, error: {e}\")\n",
    "        continue\n",
    "    for productType in products[\"products\"].keys():\n",
    "        print(f\"Generating responses for {productType} reviews using {model}...\")\n",
    "        for review in products[\"products\"][productType][\"reviews\"]:\n",
    "            try:\n",
    "                reviews_content = review[\"review_content\"]\n",
    "\n",
    "                prompt = prompt_template.format(content=reviews_content)\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                outputs = nlp(messages, max_new_tokens=256)\n",
    "                responses.append(\n",
    "                    [outputs[0][\"generated_text\"][1][\"content\"], review, model]\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed to generate response for {productType} review, error: {e}\"\n",
    "                )\n",
    "                continue\n",
    "    try:\n",
    "        # the memory is probably freed up, but to be safe we delete the nlp object and empty the cache\n",
    "        del nlp\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete nlp object, error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"rb\") as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5055457353591919\n",
      "0.7185176014900208\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, dict found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m scoreArr\u001b[38;5;241m.\u001b[39mappend([])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m similarity_metrics:\n\u001b[1;32m---> 10\u001b[0m     scoreArr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     print(f\"Failed to calculate similarity score, error: {e}\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m, in \u001b[0;36mcompare_product_review_similarity\u001b[1;34m(original_data, llm_answer)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# String Comparison (BERTScore) between Product Title and Review Data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m review_info_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     19\u001b[0m     [\n\u001b[0;32m     20\u001b[0m         original_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgolden_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_category\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(original_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgolden_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     22\u001b[0m     ]\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m product_info_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_answer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_category\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_answer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mother_keywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m score(\n\u001b[0;32m     33\u001b[0m     [product_info_string],\n\u001b[0;32m     34\u001b[0m     [review_info_string],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(F1\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "    # try:\n",
    "    llm_response = response[0]\n",
    "    if not isinstance(llm_response, dict):\n",
    "        llm_response = json.loads(\n",
    "            llm_response.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "        )\n",
    "    scoreArr.append([])\n",
    "    for metric in similarity_metrics:\n",
    "        scoreArr[-1].append(metric(response[1], llm_response))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to calculate similarity score, error: {e}\")\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"response\", \"review\", \"model\"] + [\n",
    "    metric.__name__ for metric in similarity_metrics\n",
    "]\n",
    "results_df = pd.DataFrame(responses, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the barplots, we need to transform the results, each row should contain a model name, a metric name, and the similarity score\n",
    "# Then plot x-axis as model name, y-axis as similarity score, and hue as metric name\n",
    "\n",
    "# Transform the results#\n",
    "results_transformed = results_df.melt(\n",
    "    id_vars=[\"response\", \"review\", \"model\"],\n",
    "    var_name=\"similarity_metric\",\n",
    "    value_name=\"similarity_score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
