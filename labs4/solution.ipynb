{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP - Extracting product info from texts with LLM**\n",
    "\n",
    "- *Karina Tiurina*\n",
    "- *Salveen Dutt*\n",
    "- *Patryk Prusak*\n",
    "\n",
    "\n",
    "Metrics used:\n",
    "\n",
    "1. Smith-Waterman\n",
    "2. Needleman-Wunsch\n",
    "3. Levenshtein Distance\n",
    "4. Cosine Similarity\n",
    "5. Bert Score\n",
    "6. Custom Metric\n",
    "\n",
    "Models used:\n",
    "\n",
    "1. Qwen2.5-7B-Instruct\n",
    "2. Qwen2.5-1.5B\n",
    "3. gemma-2-9b\n",
    "4. Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs & Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bert_score import score\n",
    "from transformers import pipeline\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(palette=\"cubehelix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"products.json\", \"r\") as file:\n",
    "    products = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main thing: on the battery itself, driving calmly, not exceeding 30 km/h, you can do 60-70km.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[\"products\"][\"Electric Bike\"][\"reviews\"][0][\"review_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_product_review_similarity(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates a similarity score between a review and a product based on\n",
    "    categories, brands, and keywords, including a comparison of full product\n",
    "    title with review information using BERTScore.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # String Comparison (BERTScore) between Product Title and Review Data\n",
    "    review_info_string = \" \".join(\n",
    "        [\n",
    "            review_data.get(\"product category\"),\n",
    "            review_data.get(\"brand\"),\n",
    "            \" \".join(review_data.get(\"other keywords\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    product_info_string = \" \".join(\n",
    "        [\n",
    "            product_data.get(\"product category\"),\n",
    "            product_data.get(\"brand\"),\n",
    "            \" \".join(product_data.get(\"other keywords\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        [product_info_string],\n",
    "        [review_info_string],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    print(F1.mean().item())\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def bert_score(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates the BERTScore between a review and a product based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = review_data.get(\"other keywords\")\n",
    "    product_keywords = product_data.get(\"other keywords\")\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(\n",
    "        [review_keywords],\n",
    "        [product_keywords],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    similarity_score += F1.mean().item() * 0.2\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def cosine_similarity(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a review and a product based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = review_data.get(\"other keywords\")\n",
    "    product_keywords = product_data.get(\"other keywords\")\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Cosine Similarity\n",
    "    similarity_score += (\n",
    "        bitsandbytes.cosine_similarity(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def levenshtein_distance(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between a review and a product based on\n",
    "    categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = review_data.get(\"other keywords\")\n",
    "    product_keywords = product_data.get(\"other keywords\")\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Levenshtein Distance\n",
    "    similarity_score += (\n",
    "        bitsandbytes.levenshtein_distance(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def needleman_wunsch(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates the Needleman-Wunsch similarity between a review and a product\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = review_data.get(\"other keywords\")\n",
    "    product_keywords = product_data.get(\"other keywords\")\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Needleman-Wunsch Similarity\n",
    "    similarity_score += (\n",
    "        bitsandbytes.needleman_wunsch(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)\n",
    "\n",
    "\n",
    "def smith_waterman(review_data, product_data):\n",
    "    \"\"\"\n",
    "    Calculates the Smith-Waterman similarity between a review and a product\n",
    "    based on categories, brands, and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # Category Matching (Highest weight)\n",
    "    if review_data.get(\"product category\") == product_data.get(\"product category\"):\n",
    "        similarity_score += 0.5\n",
    "\n",
    "    # Brand Matching (Medium weight)\n",
    "    if review_data.get(\"brand\") == product_data.get(\"brand\"):\n",
    "        similarity_score += 0.3\n",
    "\n",
    "    # Keyword Matching (Lowest weight)\n",
    "    review_keywords = review_data.get(\"other keywords\")\n",
    "    product_keywords = product_data.get(\"other keywords\")\n",
    "\n",
    "    if not review_keywords or not product_keywords:\n",
    "        return 0\n",
    "\n",
    "    # Calculate Smith-Waterman Similarity\n",
    "    similarity_score += (\n",
    "        bitsandbytes.smith_waterman(review_keywords, product_keywords) * 0.2\n",
    "    )\n",
    "\n",
    "    return round(min(1, similarity_score) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_metrics = [\n",
    "    smith_waterman,\n",
    "    needleman_wunsch,\n",
    "    levenshtein_distance,\n",
    "    cosine_similarity,\n",
    "    bert_score,\n",
    "    compare_product_review_similarity,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant, helping in understanding of reviews. Carefully read the review:\n",
    "{content}\n",
    "\n",
    "Return json format with the following JSON schema:\n",
    "\n",
    "{{\n",
    "        \"product category\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"Electric bicycle\", \"Refrigirator\", \"The Blocks\", \"Others\"]\n",
    "        }},\n",
    "        \"brand\": {{\n",
    "            \"type\": \"string\" or N/A\n",
    "        }},\n",
    "        \"other keywords\": {{\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {{\n",
    "                \"type\": \"string\"\n",
    "            }}\n",
    "        }},\n",
    "\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"google/gemma-2-9b\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for Refrigirator reviews using Qwen/Qwen2.5-7B-Instruct...\n",
      "Generating responses for The LEGO reviews using Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for Electric Bike reviews using Qwen/Qwen2.5-1.5B...\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "        nlp = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            model_kwargs={\n",
    "                \"quantization_config\": {\n",
    "                    \"load_in_4bit\": True,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model}, error: {e}\")\n",
    "        continue\n",
    "    for productType in products[\"products\"].keys():\n",
    "        print(f\"Generating responses for {productType} reviews using {model}...\")\n",
    "        for review in products[\"products\"][productType][\"reviews\"]:\n",
    "            try:\n",
    "                reviews_content = review[\"review_content\"]\n",
    "\n",
    "                prompt = prompt_template.format(content=reviews_content)\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                response = nlp(messages, max_length=1024, num_return_sequences=1)\n",
    "\n",
    "                responses.append(\n",
    "                    [response[0][\"generated_text\"][1][\"content\"], review, model]\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed to generate response for {productType} review, error: {e}\"\n",
    "                )\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n    \"product category\": \"Electric bicycle\",\\n    \"brand\": \"N/A\",\\n    \"other keywords\": [\\n        \"battery performance\",\\n        \"speed limit\",\\n        \"distance range\"\\n    ]\\n}',\n",
       " {'review_content': 'The main thing: on the battery itself, driving calmly, not exceeding 30 km/h, you can do 60-70km.',\n",
       "  'golden_answer': {'product_category': 'Electic bicycle',\n",
       "   'other_keywords': ['battery', 'calmly']}},\n",
       " 'Qwen/Qwen2.5-7B-Instruct']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.parse(responses[0].replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in responses:\n",
    "    try:\n",
    "        llm_response = response[0]\n",
    "        llm_response = json.parse(llm_response.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\"))\n",
    "        score =[metric(llm_response, response[1]) for metric in similarity_metrics]\n",
    "        response = response + score\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate similarity score, error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"response\", \"review\", \"model\"] + [metric.__name__ for metric in similarity_metrics]\n",
    "results_df = pd.DataFrame(responses, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the barplots, we need to transform the results, each row should contain a model name, a metric name, and the similarity score\n",
    "# Then plot x-axis as model name, y-axis as similarity score, and hue as metric name\n",
    "\n",
    "\n",
    "results_transformed = results_df.melt(\n",
    "    id_vars=[\"response\", \"review\", \"model\"],\n",
    "    var_name=\"similarity_metric\",\n",
    "    value_name=\"similarity_score\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
